{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val filePath = \"NYPD_Complaint_Data_Historic.csv\"\n",
        "\n",
        "val rawDF = spark.read\n",
        "  .option(\"header\", \"true\")\n",
        "  .option(\"multiLine\", \"true\")\n",
        "  .option(\"inferSchema\", \"true\")\n",
        "  .option(\"escape\", \"\\\"\")\n",
        "  .csv(filePath)\n",
        "\n",
        "z.show(rawDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val baseDF = rawDF.select(\n",
        "  \"cmplnt_num\",\n",
        "  \"cmplnt_fr_dt\",\n",
        "  \"cmplnt_fr_tm\",\n",
        "  \"cmplnt_to_dt\",\n",
        "  \"cmplnt_to_tm\",\n",
        "  \"rpt_dt\",\n",
        "  \"ofns_desc\",\n",
        "  \"law_cat_cd\",\n",
        "  \"boro_nm\",\n",
        "  \"susp_age_group\",\n",
        "  \"susp_race\",\n",
        "  \"susp_sex\",\n",
        "  \"latitude\",\n",
        "  \"longitude\",\n",
        "  \"vic_age_group\",\n",
        "  \"vic_race\",\n",
        "  \"vic_sex\",\n",
        ")\n",
        "\n",
        "baseDF.cache().count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "z.show(baseDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.types._\n",
        "\n",
        "val baseDF1 = baseDF.na.drop().withColumn(\"cmplnt_fr_dt\", to_date(col(\"cmplnt_fr_dt\"), \"MM/dd/yyyy\")).\n",
        "                      withColumn(\"cmplnt_to_dt\",  to_date(col(\"cmplnt_fr_dt\"), \"MM/dd/yyyy\")).\n",
        "                      withColumn(\"rpt_dt\",  to_date(col(\"cmplnt_fr_dt\"), \"MM/dd/yyyy\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "z.show(baseDF1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val startDate = \"2020-01-01\"\n",
        "val endDate = \"2022-12-31\"\n",
        "\n",
        "val baseDF2 = baseDF1.filter($\"rpt_dt\" >= startDate && $\"rpt_dt\" <= endDate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "baseDF2.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val allowedAges = Seq(\"<18\", \"18-24\", \"25-44\", \"45-64\", \"65+\")\n",
        "val baseDF3 = baseDF2.withColumn(\"susp_age_group\", when(col(\"susp_age_group\").isin(allowedAges: _*), col(\"susp_age_group\")).otherwise(null))\n",
        "                        .withColumn(\"vic_age_group\", when(col(\"vic_age_group\").isin(allowedAges: _*), col(\"vic_age_group\")).otherwise(null))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val columns = baseDF3.columns\n",
        "val replacementValues = Seq(\"(null)\", \"UNKNOWN\")\n",
        "val baseDF4 = columns.foldLeft(baseDF3) { (tempDF, colName) =>\n",
        "  tempDF.withColumn(colName, when(col(colName).isin(replacementValues: _*), null).otherwise(col(colName)))\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "z.show(baseDF4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "baseDF4.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.sql.types._\n",
        "\n",
        "val baseDF5 = baseDF4.withColumn(\"latitude\", col(\"latitude\").cast(DoubleType))\n",
        "                .withColumn(\"longitude\", col(\"longitude\").cast(DoubleType))\n",
        "                .withColumn(\"cmplnt_fr_tm\", date_format(to_timestamp(col(\"cmplnt_fr_tm\"), \"HH:mm:ss\"), \"HH:mm:ss\"))\n",
        "                .withColumn(\"cmplnt_to_tm\", date_format(to_timestamp(col(\"cmplnt_to_tm\"), \"HH:mm:ss\"), \"HH:mm:ss\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "z.show(baseDF5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "baseDF5.printSchema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "z.show(baseDF5.select(\"boro_nm\").distinct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "z.show(baseDF5.select(\"vic_age_group\").distinct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "z.show(baseDF5.select(\"susp_race\").distinct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "z.show(baseDF5.select(\"ofns_desc\").distinct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val filePath = \"zipcodes.csv\"\n",
        "\n",
        "val zipcodes = spark.read\n",
        "  .option(\"header\", \"true\")\n",
        "  .option(\"multiLine\", \"true\")\n",
        "  .option(\"inferSchema\", \"false\")\n",
        "  .option(\"escape\", \"\\\"\")\n",
        "  .csv(filePath)\n",
        "\n",
        "z.show(zipcodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val zipcodesDF = zipcodes.withColumn(\"LAT\", col(\"LAT\").cast(DoubleType))\n",
        "                         .withColumn(\"LNG\", col(\"LNG\").cast(DoubleType))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.sql.DataFrame\n",
        "import org.apache.spark.sql.functions.broadcast\n",
        "import org.apache.spark.sql.functions.udf\n",
        "\n",
        "def haversine(lat1: Double, lon1: Double, lat2: Double, lon2: Double): Double = {\n",
        "    val latDistance = Math.toRadians(lat1 - lat2)\n",
        "    val lngDistance = Math.toRadians(lon1 - lon2)\n",
        "    val sinLat = Math.sin(latDistance / 2)\n",
        "    val sinLng = Math.sin(lngDistance / 2)\n",
        "    val a = sinLat * sinLat +\n",
        "    (Math.cos(Math.toRadians(lat1)) *\n",
        "        Math.cos(Math.toRadians(lat2)) *\n",
        "        sinLng * sinLng)\n",
        "    val c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a))\n",
        "    (6371 * c).toInt\n",
        "}\n",
        "\n",
        "val haversineUDF = udf(haversine _)\n",
        "\n",
        "val broadcastUSZipDf = sc.broadcast(zipcodesDF.collect())\n",
        "\n",
        "val getNearestZipCode = udf((lat1: Double, lon1: Double) => {\n",
        "    var minDistance = Double.MaxValue\n",
        "    var nearestZipCode = \"\"\n",
        "\n",
        "    broadcastUSZipDf.value.foreach { row =>\n",
        "        val lat2 = row.getAs[Double](\"LAT\")\n",
        "        val lon2 = row.getAs[Double](\"LNG\")\n",
        "        val distance = haversine(lat1, lon1, lat2, lon2)\n",
        "        if (distance < minDistance) {\n",
        "            minDistance = distance\n",
        "            nearestZipCode = row.getAs[String](\"ZIP\")\n",
        "        }\n",
        "    }\n",
        "    nearestZipCode\n",
        "})\n",
        "\n",
        "val baseDF6 = baseDF5.withColumn(\"zipcode\", getNearestZipCode(col(\"latitude\"), col(\"longitude\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "z.show(baseDF6)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "BDAD"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
