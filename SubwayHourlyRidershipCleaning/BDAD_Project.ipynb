{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Ingesting the Main MTA subway ridership file.\n",
        "\n",
        "val filePath = \"bdad_proj/MTA_Subway_Hourly_Ridership__Beginning_February_2022_202404.csv\"\n",
        "\n",
        "val MTADf = spark.read\n",
        "  .option(\"header\", \"true\")\n",
        "  .option(\"multiLine\", \"true\")\n",
        "  .option(\"inferSchema\", \"true\")\n",
        "  .option(\"escape\", \"\\\"\")\n",
        "  .csv(filePath)\n",
        "z.show(MTADf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "//Extracting the lats and longs in my main MTA dataset for generating its associated zipcodes\n",
        "\n",
        "val columnsToSelect = Seq(\"latitude\", \"longitude\")\n",
        "\n",
        "// Created a dataset with distinct lats and longs\n",
        "val LatLngDf = MTADf.select(columnsToSelect.head, columnsToSelect.tail: _*).distinct()\n",
        "\n",
        "\n",
        "z.show(LatLngDf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Using a dataset of US zipcodes with associated Latitude and Longitudes\n",
        "val filePath_USZip = \"bdad_proj/USZipCodes.csv\"\n",
        "\n",
        "val USZipDf = spark.read\n",
        "  .option(\"header\", \"true\")\n",
        "  .option(\"multiLine\", \"true\")\n",
        "  .option(\"inferSchema\", \"true\")\n",
        "  .option(\"escape\", \"\\\"\")\n",
        "  .csv(filePath_USZip)\n",
        "  \n",
        "println(USZipDf.count())\n",
        "  \n",
        "z.show(USZipDf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Used a dataset with all the zipcodes in New York City. This helps us cut down on the USzip codes dataframe size\n",
        "\n",
        "val zip_filePath = \"bdad_proj/NYCZipCodes.csv\"\n",
        "\n",
        "val ZipDf = spark.read\n",
        "  .option(\"header\", \"true\")\n",
        "  .option(\"multiLine\", \"true\")\n",
        "  .option(\"inferSchema\", \"true\")\n",
        "  .option(\"escape\", \"\\\"\")\n",
        "  .csv(zip_filePath)\n",
        "\n",
        "z.show(ZipDf)\n",
        "println(ZipDf.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n",
        "// Performing an inner join between ZipDf and USZipDf on the ZIP code column\n",
        "val joinedDF = USZipDf.join(ZipDf, USZipDf(\"ZIP\") === ZipDf(\"ZIPCODES\"), \"inner\")\n",
        "\n",
        "// Selecting the columns you want to keep from the joined DataFrame\n",
        "val result_Zip_DF = joinedDF.select(\"ZIP\", \"LAT\", \"LNG\")\n",
        "\n",
        "// Showing the resulting DataFrame\n",
        "z.show(result_Zip_DF)\n",
        "println(result_Zip_DF.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Removing the unnecessary columns like transit_mode, counties and NYS Municipal Boundaries.\n",
        "\n",
        "val filteredMTADf = MTADf.drop(\"transit_mode\", \"ridership\", \"transfers\", \"Counties\", \"NYS Municipal Boundaries\", \"Georeference\", \"New York Zip Codes\")\n",
        "z.show(filteredMTADf.limit(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Cleaning the timestamp and extracting time and date from the timestamp data\n",
        "\n",
        "import org.apache.spark.sql.functions.{to_date, date_format, to_timestamp}\n",
        "\n",
        "\n",
        "val filteredMTADf_1 = filteredMTADf\n",
        "                .withColumn(\"transit_timestamp\", to_timestamp($\"transit_timestamp\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
        "                .withColumn(\"transit_date\", to_date($\"transit_timestamp\"))\n",
        "                .withColumn(\"transit_time\", date_format($\"transit_timestamp\",\"hh:mm:ss a\" ))\n",
        "\n",
        "z.show(filteredMTADf_1.limit(100))\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Using Haversine formula to generated zipcodes associated with the unique lat and longs present in the main datset using the US Zip codes datset as a reference\n",
        "\n",
        "import org.apache.spark.sql.expressions.Window\n",
        "\n",
        "def haversine(lat1: Double, lon1: Double, lat2: Double, lon2: Double): Double = {\n",
        "    val latDistance = Math.toRadians(lat1 - lat2)\n",
        "    val lngDistance = Math.toRadians(lon1 - lon2)\n",
        "    val sinLat = Math.sin(latDistance / 2)\n",
        "    val sinLng = Math.sin(lngDistance / 2)\n",
        "    val a = sinLat * sinLat +\n",
        "    (Math.cos(Math.toRadians(lat1)) *\n",
        "        Math.cos(Math.toRadians(lat2)) *\n",
        "        sinLng * sinLng)\n",
        "    val c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a))\n",
        "    (6371 * c).toInt\n",
        "}\n",
        "\n",
        "val getNearestZipCodes = udf((lat1: Double, lon1: Double, lat2: Double, lon2: Double) =>\n",
        "    haversine(lat1, lon1, lat2, lon2)\n",
        ")\n",
        "\n",
        "val radius = 0.5\n",
        "\n",
        "val broadcastUSZipDf = broadcast(result_Zip_DF)\n",
        "\n",
        "val result = LatLngDf.crossJoin(broadcastUSZipDf)\n",
        "    .withColumn(\"distance\", getNearestZipCodes($\"LAT\", $\"LNG\", $\"latitude\", $\"longitude\"))\n",
        "    .filter($\"distance\" < radius)\n",
        "    .groupBy($\"latitude\", $\"longitude\")\n",
        "    .agg(min($\"ZIP\").as(\"nearest_zipcode\"))\n",
        "    \n",
        "\n",
        "\n",
        "result.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Performing an inner join between ZipDf and USZipDf on the ZIP code column to attach the associated zipcodes to the main dataset\n",
        "val joinedDF = filteredMTADf_1.join(result, filteredMTADf_1(\"latitude\") === result(\"latitude\") && filteredMTADf_1(\"longitude\") === result(\"longitude\") , \"left\")\n",
        "\n",
        "// Show the resulting DataFrame\n",
        "z.show(joinedDF)\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Further dropping Lat and Long data because now it is not needed for the future analytics purpose\n",
        "\n",
        "val dfWithoutDuplicates = joinedDF.drop(\"latitude\", \"longitude\")\n",
        "z.show(dfWithoutDuplicates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Saving the final cleaned dataset into hdfs\n",
        "\n",
        "val outputPath = \"hdfs:///user/vp2359_nyu_edu/bdad_proj/ZipCodedLatsLongs.csv\"\n",
        "\n",
        "dfWithoutDuplicates.write\n",
        ".option(\"header\", \"true\") // Include column headers\n",
        ".csv(outputPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Extracting the cleaned dataset back into Zeppelin\n",
        "\n",
        "val MTA_Final_Df = spark.read.option(\"header\", \"true\").option(\"multiLine\", \"true\")\n",
        "  .option(\"inferSchema\", \"true\")\n",
        "  .option(\"escape\", \"\\\"\").csv(\"bdad_proj/ZipCodedLatsLongs.csv/part-*.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n",
        "val MTA_Final = MTA_Final_Df.withColumnRenamed(\"nearest_zipcode\", \"zipcode\")\n",
        "z.show(MTA_Final.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Profiling dataset\n",
        "\n",
        "// Seeing the ridership distribution among the boroughs\n",
        "val groupByBorough = MTA_Final.groupBy(\"borough\").count()\n",
        "z.show(groupByBorough)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Dataset count\n",
        "\n",
        "println(MTA_Final.count())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "python",
      "pygments_lexer": "scala",
      "version": "3.12.0"
    },
    "name": "BDAD_Project"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
